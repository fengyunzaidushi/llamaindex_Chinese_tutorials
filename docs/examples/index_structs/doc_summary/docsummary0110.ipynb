{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "from llama_index.indices.loading import load_index_from_storage\n",
    "from llama_index import StorageContext\n",
    "\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from read import get_filelisform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_file):\n",
    "    from pathlib import Path\n",
    "    from llama_index import download_loader\n",
    "\n",
    "    PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "    loader = PDFReader()\n",
    "    documents = loader.load_data(file=Path('./data/自律修炼手册.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(directory,format):\n",
    "    pdf_files = get_filelisform(directory,format)\n",
    "    # Load all wiki documents\n",
    "    city_docs = []\n",
    "    for file in pdf_files:\n",
    "        docs = SimpleDirectoryReader(\n",
    "            input_files=[file]\n",
    "        ).load_data()\n",
    "        title = file.split(':')[0]\n",
    "        docs[0].doc_id = title\n",
    "        city_docs.extend(docs)\n",
    "    return city_docs,pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_base1 = os.environ['openai_api_base1']\n",
    "api_key1 = os.environ['openai_api_key1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-A2IJsOOcVEjwYlph31BaB9B48cBf459bA4D3212b48D2Ed83'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://aigc789.top/v1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_base1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# LLM (gpt-3.5-turbo)\n",
    "models= [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\", \"gpt-4-1106-preview\", \"gpt-3.5-turbo-16k\", \"gpt-4-0613\"]\n",
    "system_prompt = \"Always respond in Chinese\"\n",
    "chatgpt = OpenAI(temperature=0, model=models[1], api_base=api_base1, api_key=api_key1,\n",
    "                system_prompt=system_prompt)\n",
    "\n",
    "persist_dir = \"index-pdf-zilv\"\n",
    "context_window= 16385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = OpenAI(temperature=0, model=models[1], api_base=api_base1, api_key=api_key1,\n",
    "                system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_docs,pdf_files = get_docs('./data/表达','.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "service_context = ServiceContext.from_defaults(llm=chatgpt,\n",
    "                                                chunk_size=1024,\n",
    "                                                context_window = context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=16385, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=OpenAIEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f58f8639750>, additional_kwargs={}, api_key='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', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f58f8639750>, id_func=<function default_id_func at 0x7f590192e3b0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7f597846db10>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f58f8639750>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "doc_summary_index = load_index_from_storage(storage_context=storage_context,service_context=service_context)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = doc_summary_index.as_query_engine(\n",
    "            response_mode=\"tree_summarize\", use_async=True\n",
    "        )\n",
    "question = \"如何进行有效的表达\"\n",
    "\n",
    "response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过设问法，可以让听众主动靠近你、接纳你。首先抛出一个跟现场听众都有关的问题，然后再说你要讲的内容。这样可以让听众更容易理解并接受你的表达。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.indices.document_summary import (\n",
    "    DocumentSummaryIndexLLMRetriever,\n",
    ")\n",
    "\n",
    "retriever = DocumentSummaryIndexLLMRetriever(\n",
    "    doc_summary_index,\n",
    "    # choice_select_prompt=None,\n",
    "    # choice_batch_size=10,\n",
    "    # choice_top_k=1,\n",
    "    # format_node_batch_fn=None,\n",
    "    # parse_choice_select_answer_fn=None,\n",
    "    service_context=service_context,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m/mnt/sda/github/12yue/llama_index/docs/examples/index_structs/doc_summary/docsummary0110.ipynb 单元格 18\u001B[0m line \u001B[0;36m1\n\u001B[0;32m----> <a href='vscode-notebook-cell:/mnt/sda/github/12yue/llama_index/docs/examples/index_structs/doc_summary/docsummary0110.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m retrieved_nodes \u001B[39m=\u001B[39m retriever\u001B[39m.\u001B[39;49mretrieve(question)\n",
      "File \u001B[0;32m/mnt/sda/github/12yue/llama_index/llama_index/core/base_retriever.py:54\u001B[0m, in \u001B[0;36mBaseRetriever.retrieve\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcallback_manager\u001B[39m.\u001B[39mas_trace(\u001B[39m\"\u001B[39m\u001B[39mquery\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[1;32m     50\u001B[0m     \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcallback_manager\u001B[39m.\u001B[39mevent(\n\u001B[1;32m     51\u001B[0m         CBEventType\u001B[39m.\u001B[39mRETRIEVE,\n\u001B[1;32m     52\u001B[0m         payload\u001B[39m=\u001B[39m{EventPayload\u001B[39m.\u001B[39mQUERY_STR: query_bundle\u001B[39m.\u001B[39mquery_str},\n\u001B[1;32m     53\u001B[0m     ) \u001B[39mas\u001B[39;00m retrieve_event:\n\u001B[0;32m---> 54\u001B[0m         nodes \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_retrieve(query_bundle)\n\u001B[1;32m     55\u001B[0m         retrieve_event\u001B[39m.\u001B[39mon_end(\n\u001B[1;32m     56\u001B[0m             payload\u001B[39m=\u001B[39m{EventPayload\u001B[39m.\u001B[39mNODES: nodes},\n\u001B[1;32m     57\u001B[0m         )\n\u001B[1;32m     58\u001B[0m \u001B[39mreturn\u001B[39;00m nodes\n",
      "File \u001B[0;32m/mnt/sda/github/12yue/llama_index/llama_index/indices/document_summary/retrievers.py:88\u001B[0m, in \u001B[0;36mDocumentSummaryIndexLLMRetriever._retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[39m# call each batch independently\u001B[39;00m\n\u001B[1;32m     83\u001B[0m raw_response \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_service_context\u001B[39m.\u001B[39mllm\u001B[39m.\u001B[39mpredict(\n\u001B[1;32m     84\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_choice_select_prompt,\n\u001B[1;32m     85\u001B[0m     context_str\u001B[39m=\u001B[39mfmt_batch_str,\n\u001B[1;32m     86\u001B[0m     query_str\u001B[39m=\u001B[39mquery_str,\n\u001B[1;32m     87\u001B[0m )\n\u001B[0;32m---> 88\u001B[0m raw_choices, relevances \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_parse_choice_select_answer_fn(\n\u001B[1;32m     89\u001B[0m     raw_response, \u001B[39mlen\u001B[39;49m(summary_nodes)\n\u001B[1;32m     90\u001B[0m )\n\u001B[1;32m     91\u001B[0m choice_idxs \u001B[39m=\u001B[39m [choice \u001B[39m-\u001B[39m \u001B[39m1\u001B[39m \u001B[39mfor\u001B[39;00m choice \u001B[39min\u001B[39;00m raw_choices]\n\u001B[1;32m     93\u001B[0m choice_summary_ids \u001B[39m=\u001B[39m [summary_ids_batch[ci] \u001B[39mfor\u001B[39;00m ci \u001B[39min\u001B[39;00m choice_idxs]\n",
      "File \u001B[0;32m/mnt/sda/github/12yue/llama_index/llama_index/indices/utils.py:105\u001B[0m, in \u001B[0;36mdefault_parse_choice_select_answer_fn\u001B[0;34m(answer, num_choices, raise_error)\u001B[0m\n\u001B[1;32m     99\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m    101\u001B[0m             \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mInvalid answer line: \u001B[39m\u001B[39m{\u001B[39;00manswer_line\u001B[39m}\u001B[39;00m\u001B[39m. \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    102\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39mAnswer line must be of the form: \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    103\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39manswer_num: <int>, answer_relevance: <float>\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    104\u001B[0m         )\n\u001B[0;32m--> 105\u001B[0m answer_num \u001B[39m=\u001B[39m \u001B[39mint\u001B[39m(line_tokens[\u001B[39m0\u001B[39;49m]\u001B[39m.\u001B[39;49msplit(\u001B[39m\"\u001B[39;49m\u001B[39m:\u001B[39;49m\u001B[39m\"\u001B[39;49m)[\u001B[39m1\u001B[39;49m]\u001B[39m.\u001B[39mstrip())\n\u001B[1;32m    106\u001B[0m \u001B[39mif\u001B[39;00m answer_num \u001B[39m>\u001B[39m num_choices:\n\u001B[1;32m    107\u001B[0m     \u001B[39mcontinue\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(retrieved_nodes)\n",
    "\n",
    "print(retrieved_nodes[0].node.get_text())\n",
    "\n",
    "# use retriever as part of a query engine\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=16385, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=OpenAIEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f58f8639750>, additional_kwargs={}, api_key='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', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f58f8639750>, id_func=<function default_id_func at 0x7f590192e3b0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7f597846db10>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f58f8639750>)\n"
     ]
    }
   ],
   "source": [
    "print(doc_summary_index.service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_nodes = retriever.retrieve(\"如何进行有效的表达\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(len(retrieved_nodes))\n",
    "\n",
    "print(retrieved_nodes[0].score)\n",
    "print(retrieved_nodes[0].node.get_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retriever as part of a query engine\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.document_summary import (\n",
    "    DocumentSummaryIndexEmbeddingRetriever,\n",
    ")\n",
    "\n",
    "retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    doc_summary_index,\n",
    "    # similarity_top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去讨好别人？我可不干， 而且我也不会说那些虚头巴脑的话。 ”  还\n",
      "有同学私下会嘀咕： “我又不是他们肚子里的蛔虫， 没那么强的同\n",
      "理心，怎么办啊？”  \n",
      "  \n",
      "放心，这些我都想到了，所以我不是教你怎么去迎合讨好，而是\n",
      "教你一套具体的工具，让你的听众主动靠近你、接纳你。  \n",
      "  \n",
      "果核：设问法  \n",
      "  \n",
      "请注意，这一讲的果核是：好的当众表达，不是迎合讨好听众，\n",
      "而是先抛出一个跟现场听众都有关的问题， 然后再说你要讲的内\n",
      "容。我给这个方法起了个名字叫 ——设问法。  \n",
      "举个例子，比如明天要开周例会，你重点想汇报两个工作。一开\n",
      "始可能是这么说的：  \n",
      "1.我做了上半年行业优秀案例分析。  \n",
      "2.还做了用户需求调研和分析。  \n",
      "虽然听着挺专业，也知道你在说什么。但听的人并不知道跟自己\n",
      "有什么关系，我为什么要听？那如果试着把他们变成设问，你再\n",
      "听听看。  \n",
      "我上周做了不少工作，今天主要想汇报三个重点内容：  \n",
      "首先，上半年咱们的几个友商都成了国潮网红，销售额大增，怎\n",
      "么学习他们的经验？为此我专门做了上半年行业内优秀案例分\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(retrieved_nodes[0].node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retriever as part of a query engine\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过使用设问法可以进行有效的表达。设问法是一种技巧，通过先提出一个与听众相关的问题，然后再介绍要讲的内容。这样可以引起听众的兴趣和共鸣，使他们更愿意倾听你的讲话。在进行表达时，可以将要传达的信息转化为问题，并在介绍时提出这些问题，以吸引听众的注意力和参与度。这种方法可以帮助你与听众建立更好的连接，使他们更主动地靠近你并接受你的观点。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# query\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用设问法可以进行有效的表达。设问法是一种技巧，通过先提出一个与听众相关的问题，然后再介绍要讲的内容。这样可以引起听众的兴趣和共鸣，使他们更愿意倾听你的讲话。在进行表达时，可以将要传达的信息转化为问题，并在介绍时提出这些问题，以吸引听众的注意力和参与度。这种方法可以帮助你与听众建立更好的连接，使他们更主动地靠近你并接受你的观点。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
