#!/usr/bin/env python
# coding: utf-8

# <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/prompts/prompt_mixin.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# # Accessing/Customizing Prompts within Higher-Level Modules
# 
# LlamaIndex contains a variety of higher-level modules (query engines, response synthesizers, retrievers, etc.), many of which make LLM calls + use prompt templates.
# 
# This guide shows how you can 1) access the set of prompts for any module (including nested) with `get_prompts`, and 2) update these prompts easily with `update_prompts`.

# If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.

#('pip install llama-index')

import os
import openai

os.environ["OPENAI_API_KEY"] = "sk-..."
openai.api_key = os.environ["OPENAI_API_KEY"]

import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

from llama_index import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    load_index_from_storage,
    StorageContext,
)
from IPython.#display import Markdown, #display

# ## Setup: Load Data, Build Index, and Get Query Engine
# 
# Here we build a vector index over a toy dataset (PG's essay), and access the query engine.
# 
# The query engine is a simple RAG pipeline consisting of top-k retrieval + LLM synthesis.

# Download Data

#("mkdir -p 'data/paul_graham/'")
#("wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'")

# load documents
documents = SimpleDirectoryReader("./data/paul_graham/").load_data()

index = VectorStoreIndex.from_documents(documents)

# set Logging to DEBUG for more detailed outputs
query_engine = index.as_query_engine(response_mode="tree_summarize")

# define prompt viewing function
def #display_prompt_dict(prompts_dict):
    for k, p in prompts_dict.items():
        text_md = f"**Prompt Key**: {k}<br>" f"**Text:** <br>"
        #display(Markdown(text_md))
        print(p.get_template())
        #display(Markdown("<br><br>"))

# ## Accessing Prompts
# 
# Here we get the prompts from the query engine. Note that *all* prompts are returned, including ones used in sub-modules in the query engine. This allows you to centralize a view of these prompts!

prompts_dict = query_engine.get_prompts()

#display_prompt_dict(prompts_dict)

# #### Checking `get_prompts` on Response Synthesizer
# 
# You can also call `get_prompts` on the underlying response synthesizer, where you'll see the same list.

prompts_dict = query_engine.response_synthesizer.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Checking `get_prompts` with a different response synthesis strategy
# 
# Here we try the default `compact` method.
# 
# We'll see that the set of templates used are different; a QA template and a refine template.

# set Logging to DEBUG for more detailed outputs
query_engine = index.as_query_engine(response_mode="compact")

prompts_dict = query_engine.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Put into query engine, get response

response = query_engine.query("What did the author do growing up?")
print(str(response))

# ## Customize the prompt
# 
# You can also update/customize the prompts with the `update_prompts` function. Pass in arg values with the keys equal to the keys you see in the prompt dictionary.
# 
# Here we'll change the summary prompt to use Shakespeare.

from llama_index.prompts import PromptTemplate

# reset
query_engine = index.as_query_engine(response_mode="tree_summarize")

# shakespeare!
new_summary_tmpl_str = (
    "Context information is below.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given the context information and not prior knowledge, "
    "answer the query in the style of a Shakespeare play.\n"
    "Query: {query_str}\n"
    "Answer: "
)
new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)

query_engine.update_prompts(
    {"response_synthesizer:summary_template": new_summary_tmpl}
)

prompts_dict = query_engine.get_prompts()

#display_prompt_dict(prompts_dict)

response = query_engine.query("What did the author do growing up?")
print(str(response))

# ## Accessing Prompts from Other Modules
# 
# Here we take a look at some other modules: query engines, routers/selectors, evaluators, and others.

from llama_index.query_engine import (
    RouterQueryEngine,
    FLAREInstructQueryEngine,
)
from llama_index.selectors import LLMMultiSelector
from llama_index.evaluation import FaithfulnessEvaluator, DatasetGenerator
from llama_index.postprocessor import LLMRerank

# #### Analyze Prompts: Router Query Engine

# setup sample router query engine
from llama_index.tools.query_engine import QueryEngineTool

query_tool = QueryEngineTool.from_defaults(
    query_engine=query_engine, description="test description"
)

router_query_engine = RouterQueryEngine.from_defaults([query_tool])

prompts_dict = router_query_engine.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Analyze Prompts: FLARE Query Engine

flare_query_engine = FLAREInstructQueryEngine(query_engine)

prompts_dict = flare_query_engine.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Analyze Prompts: LLMMultiSelector

from llama_index.selectors.llm_selectors import LLMSingleSelector

selector = LLMSingleSelector.from_defaults()

prompts_dict = selector.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Analyze Prompts: FaithfulnessEvaluator

evaluator = FaithfulnessEvaluator()

prompts_dict = evaluator.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Analyze Prompts: DatasetGenerator

dataset_generator = DatasetGenerator.from_documents(documents)

prompts_dict = dataset_generator.get_prompts()
#display_prompt_dict(prompts_dict)

# #### Analyze Prompts: LLMRerank

llm_rerank = LLMRerank()

prompts_dict = dataset_generator.get_prompts()
#display_prompt_dict(prompts_dict)

