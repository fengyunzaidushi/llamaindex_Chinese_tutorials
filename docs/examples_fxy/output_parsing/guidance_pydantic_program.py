#!/usr/bin/env python
# coding: utf-8

# <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/output_parsing/guidance_pydantic_program.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# # Guidance Pydantic Program

# Generate structured data with [**guidance**](https://github.com/microsoft/guidance) via LlamaIndex.  
# 
# 
# With guidance, you can guarantee the output structure is correct by *forcing* the LLM to output desired tokens.  
# This is especialy helpful when you are using lower-capacity model (e.g. the current open source models), which otherwise would struggle to generate valid output that fits the desired output schema.

# If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.

#('pip install llama-index')

from pydantic import BaseModel
from typing import List
from guidance.llms import OpenAI

from llama_index.program import GuidancePydanticProgram

# Define output schema

class Song(BaseModel):
    title: str
    length_seconds: int

class Album(BaseModel):
    name: str
    artist: str
    songs: List[Song]

# Define guidance pydantic program

program = GuidancePydanticProgram(
    output_cls=Album,
    prompt_template_str=(
        "Generate an example album, with an artist and a list of songs. Using"
        " the movie {{movie_name}} as inspiration"
    ),
    guidance_llm=OpenAI("text-davinci-003"),
    verbose=True,
)

# Run program to get structured output.  
# Text highlighted in blue is variables specified by us, text highlighted in green is generated by the LLM.

output = program(movie_name="The Shining")

# The output is a valid Pydantic object that we can then use to call functions/APIs. 

output

