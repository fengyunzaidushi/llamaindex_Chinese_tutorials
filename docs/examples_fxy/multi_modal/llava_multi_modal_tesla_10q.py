#!/usr/bin/env python
# coding: utf-8

# <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/llava_multi_modal_tesla_10q.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
# # Retrieval-Augmented Image Captioning
# 

# 
# 1. LlaVa can provide image understanding based on user prompt.
# 2. We use Unstructured to parse out the tables, and use LlamaIndex recursive retrieval to index/retrieve tables and texts.
# 3. We can leverage the image understanding from Step 1 to retrieve relevant information from knowledge base generated by Step 2 (which is indexed by LlamaIndex)
# 
# Context for LLaVA: Large Language and Vision Assistant
# * [Website](https://llava-vl.github.io/)
# * [Paper](https://arxiv.org/abs/2304.08485)
# * [Github](https://github.com/haotian-liu/LLaVA)
# * LLaVA is now supported in llama.cpp with 4-bit / 5-bit quantization support: [See here.](https://github.com/ggerganov/llama.cpp/pull/3436) [Deprecated]
# * LLaVA 13b is now supported in Replicate: [See here.](https://replicate.com/yorickvp/llava-13b)
# 
# For LlamaIndex:
# LlaVa+Replicate enables us to run image understanding locally and combine the multi-modal knowledge with our RAG knowledge base system.
# 
# TODO:
# Waiting for [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) supporting LlaVa model in python wrapper.
# So LlamaIndex can leverage `LlamaCPP` class for serving LlaVa model directly/locally.

# ## Using Replicate serving LLaVa model through LlamaIndex
# 

# ### Build and Run LLaVa models locally through Llama.cpp (Deprecated)
# 
# 1. git clone [https://github.com/ggerganov/llama.cpp.git](https://github.com/ggerganov/llama.cpp.git)
# 2. `cd llama.cpp`. Checkout llama.cpp repo for more details.
# 3. `make`
# 4. Download Llava models including `ggml-model-*` and `mmproj-model-*` from [this Hugging Face repo](https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main). Please select one model based on your own local configuration
# 5. `./llava` for checking whether llava is running locally

get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('', 'autoreload 2')

#(' pip install unstructured')

from unstructured.partition.html import partition_html
import pandas as pd

pd.set_option("#display.max_rows", None)
pd.set_option("#display.max_columns", None)
pd.set_option("#display.width", None)
pd.set_option("#display.max_colwidth", None)

# ## Perform Data Extraction from Tesla 10K file
# 
# 

# ### Extract Elements
# 
# We use Unstructured to extract table and non-table elements from the 10-K filing.

#('wget "https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&dl=1" -O tesla_2021_10k.htm')
#('wget "https://docs.google.com/uc?export=download&id=1THe1qqM61lretr9N3BmINc_NWDvuthYf" -O shanghai.jpg')
#('wget "https://docs.google.com/uc?export=download&id=1PDVCf_CzLWXNnNoRV8CFgoJxv6U0sHAO" -O tesla_supercharger.jpg')

from llama_index.readers.file.flat_reader import FlatReader
from pathlib import Path

reader = FlatReader()
docs_2021 = reader.load_data(Path("tesla_2021_10k.htm"))

from llama_index.node_parser import (
    UnstructuredElementNodeParser,
)

node_parser = UnstructuredElementNodeParser()

import os

REPLICATE_API_TOKEN = ""  # Your Relicate API token here
os.environ["REPLICATE_API_TOKEN"] = REPLICATE_API_TOKEN

import openai

OPENAI_API_TOKEN = "sk-"
openai.api_key = OPENAI_API_TOKEN  # add your openai api key here
os.environ["OPENAI_API_KEY"] = OPENAI_API_TOKEN

import os
import pickle

if not os.path.exists("2021_nodes.pkl"):
    raw_nodes_2021 = node_parser.get_nodes_from_documents(docs_2021)
    pickle.dump(raw_nodes_2021, open("2021_nodes.pkl", "wb"))
else:
    raw_nodes_2021 = pickle.load(open("2021_nodes.pkl", "rb"))

base_nodes_2021, node_mappings_2021 = node_parser.get_base_nodes_and_mappings(
    raw_nodes_2021
)

# ## Setup Recursive Retriever
# 
# Now that we've extracted tables and their summaries, we can setup a recursive retriever in LlamaIndex to query these tables.

# ### Construct Retrievers

from llama_index.retrievers import RecursiveRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index import VectorStoreIndex

# construct top-level vector index + query engine
vector_index = VectorStoreIndex(base_nodes_2021)
vector_retriever = vector_index.as_retriever(similarity_top_k=2)
vector_query_engine = vector_index.as_query_engine(similarity_top_k=2)

from llama_index.retrievers import RecursiveRetriever

recursive_retriever = RecursiveRetriever(
    "vector",
    retriever_dict={"vector": vector_retriever},
    node_dict=node_mappings_2021,
    verbose=True,
)
query_engine = RetrieverQueryEngine.from_args(recursive_retriever)

from PIL import Image
import matplotlib.pyplot as plt

imageUrl = "./tesla_supercharger.jpg"
image = Image.open(imageUrl).convert("RGB")

plt.figure(figsize=(16, 5))
plt.imshow(image)

# ### Running LLaVa model using Replicate through LlamaIndex for image understanding

from llama_index.multi_modal_llms import ReplicateMultiModal
from llama_index.schema import ImageDocument
from llama_index.multi_modal_llms.replicate_multi_modal import (
    REPLICATE_MULTI_MODAL_LLM_MODELS,
)

multi_modal_llm = ReplicateMultiModal(
    model=REPLICATE_MULTI_MODAL_LLM_MODELS["llava-13b"],
    max_new_tokens=200,
    temperature=0.1,
)

prompt = "what is the main object for tesla in the image?"

llava_response = multi_modal_llm.complete(
    prompt=prompt,
    image_documents=[ImageDocument(image_path=imageUrl)],
)

# ### Retrieve relevant information from LlamaIndex knowledge base according to LLaVa image understanding

prompt_template = "please provide relevant information about: "
rag_response = query_engine.query(prompt_template + llava_response.text)

# ### Showing final RAG image caption results from LlamaIndex

print(str(rag_response))

from PIL import Image
import matplotlib.pyplot as plt

imageUrl = "./shanghai.jpg"
image = Image.open(imageUrl).convert("RGB")

plt.figure(figsize=(16, 5))
plt.imshow(image)

# ### Retrieve relevant information from LlamaIndex for a new image

prompt = "which Tesla factory is shown in the image?"

llava_response = multi_modal_llm.complete(
    prompt=prompt,
    image_documents=[ImageDocument(image_path=imageUrl)],
)

prompt_template = "please provide relevant information about: "
rag_response = query_engine.query(prompt_template + llava_response.text)

# ### Showing final RAG image caption results from LlamaIndex

print(rag_response)

