Welcome to LlamaIndex's Chinese documentation!
===================================

LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. Itâ€™s available in Python (these docs) and Typescript.
LlamaIndexæ˜¯ä¸€ä¸ªæ•°æ®æ¡†æ¶ï¼Œç”¨äºåŸºäºLLMçš„åº”ç”¨ç¨‹åºæ‘„å–ã€ç»“æ„åŒ–å’Œè®¿é—®ç§æœ‰æˆ–é¢†åŸŸç‰¹å®šçš„æ•°æ®ã€‚å®ƒå¯åœ¨Pythonï¼ˆæœ¬æ–‡æ¡£ï¼‰å’ŒTypescriptä¸­ä½¿ç”¨ã€‚

ğŸš€ Why LlamaIndex? ä¸ºä»€ä¹ˆé€‰æ‹©LlamaIndexï¼Ÿ
LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data like Wikipedia, mailing lists, textbooks, source code and more.
LLMsæä¾›äº†äººç±»å’Œæ•°æ®ä¹‹é—´çš„è‡ªç„¶è¯­è¨€æ¥å£ã€‚å¹¿æ³›å¯ç”¨çš„æ¨¡å‹å·²ç»åœ¨å¤§é‡å…¬å¼€å¯ç”¨çš„æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¦‚ç»´åŸºç™¾ç§‘ã€é‚®ä»¶åˆ—è¡¨ã€æ•™ç§‘ä¹¦ã€æºä»£ç ç­‰ã€‚

However, while LLMs are trained on a great deal of data, they are not trained on your data, which may be private or specific to the problem youâ€™re trying to solve. Itâ€™s behind APIs, in SQL databases, or trapped in PDFs and slide decks.
ç„¶è€Œï¼Œå°½ç®¡LLMsæ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰åœ¨æ‚¨çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®å¯èƒ½æ˜¯ç§æœ‰çš„æˆ–ç‰¹å®šäºæ‚¨è¯•å›¾è§£å†³çš„é—®é¢˜ã€‚å®ƒä»¬å¯èƒ½å­˜åœ¨äºAPIåé¢ï¼Œåœ¨SQLæ•°æ®åº“ä¸­ï¼Œæˆ–è€…è¢«å›°åœ¨PDFå’Œå¹»ç¯ç‰‡ä¸­ã€‚

You may choose to fine-tune a LLM with your data, but:
æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨æ‚¨çš„æ•°æ®æ¥å¾®è°ƒLLMï¼Œä½†æ˜¯ï¼š

Training a LLM is expensive.
è®­ç»ƒä¸€ä¸ªLLMæ˜¯æ˜‚è´µçš„ã€‚

Due to the cost to train, itâ€™s hard to update a LLM with latest information.
ç”±äºåŸ¹è®­æˆæœ¬é«˜æ˜‚ï¼Œæ›´æ–°LLMçš„æœ€æ–°ä¿¡æ¯å¾ˆå›°éš¾ã€‚

Observability is lacking. When you ask a LLM a question, itâ€™s not obvious how the LLM arrived at its answer.
å¯è§‚å¯Ÿæ€§ä¸è¶³ã€‚å½“ä½ å‘ä¸€ä¸ªLLMæé—®æ—¶ï¼Œå¾ˆéš¾çœ‹å‡ºLLMæ˜¯å¦‚ä½•å¾—å‡ºç­”æ¡ˆçš„ã€‚

LlamaIndex takes a different approach called Retrieval-Augmented Generation (RAG). Instead of asking LLM to generate an answer immediately, LlamaIndex:
LlamaIndexé‡‡ç”¨ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼Œç§°ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚è€Œä¸æ˜¯è¦æ±‚LLMç«‹å³ç”Ÿæˆç­”æ¡ˆï¼ŒLlamaIndexï¼š

retrieves information from your data sources first,
é¦–å…ˆä»æ‚¨çš„æ•°æ®æºä¸­æ£€ç´¢ä¿¡æ¯

adds it to your question as context, and
å°†å…¶ä½œä¸ºèƒŒæ™¯æ·»åŠ åˆ°æ‚¨çš„é—®é¢˜ä¸­ï¼Œå¹¶è¿”å›è¯‘æ–‡

asks the LLM to answer based on the enriched prompt.
æ ¹æ®ä¸°å¯Œçš„æç¤ºï¼Œè¦æ±‚LLMè¿›è¡Œå›ç­”ã€‚

RAG overcomes all three weaknesses of the fine-tuning approach:
RAGå…‹æœäº†å¾®è°ƒæ–¹æ³•çš„ä¸‰ä¸ªå¼±ç‚¹

Thereâ€™s no training involved, so itâ€™s cheap.
æ²¡æœ‰åŸ¹è®­å‚ä¸ï¼Œæ‰€ä»¥å¾ˆä¾¿å®œã€‚

Data is fetched only when you ask for them, so itâ€™s always up to date.
æ•°æ®åªåœ¨æ‚¨è¯·æ±‚æ—¶è·å–ï¼Œå› æ­¤å§‹ç»ˆä¿æŒæœ€æ–°ã€‚

LlamaIndex can show you the retrieved documents, so itâ€™s more trustworthy.
LlamaIndexå¯ä»¥æ˜¾ç¤ºæ‚¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå› æ­¤æ›´å¯é ã€‚

LlamaIndex imposes no restriction on how you use LLMs. You can still use LLMs as auto-complete, chatbots, semi-autonomous agents, and more (see Use Cases on the left). It only makes LLMs more relevant to you.
LlamaIndexå¯¹äºæ‚¨å¦‚ä½•ä½¿ç”¨LLMsæ²¡æœ‰ä»»ä½•é™åˆ¶ã€‚æ‚¨ä»ç„¶å¯ä»¥å°†LLMsç”¨ä½œè‡ªåŠ¨å®Œæˆã€èŠå¤©æœºå™¨äººã€åŠè‡ªä¸»ä»£ç†ç­‰ï¼ˆè¯·å‚è§å·¦ä¾§çš„ä½¿ç”¨æ¡ˆä¾‹ï¼‰ã€‚å®ƒåªä¼šä½¿LLMså¯¹æ‚¨æ›´åŠ ç›¸å…³ã€‚
